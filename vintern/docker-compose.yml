version: '3.8'

services:
  # CPU version
  vintern-cpu:
    build:
      context: .
      dockerfile: Dockerfile.cpu
    container_name: vintern-cpu
    ports:
      - "11200:8000"
    environment:
      - DEVICE=cpu
    volumes:
      # Use local cache (faster - reuses downloaded model)
      # Linux/Mac: ~/.cache/huggingface
      # Windows: %USERPROFILE%\.cache\huggingface
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      # Or use Docker volume (slower first run)
      # - model-cache:/root/.cache/huggingface
    restart: unless-stopped
    profiles: [cpu]

  # GPU version
  vintern-gpu:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: vintern-gpu
    ports:
      - "11200:8000"
    environment:
      - DEVICE=cuda
    volumes:
      # Use local cache (faster - reuses downloaded model)
      # Linux/Mac: ~/.cache/huggingface
      # Windows: %USERPROFILE%\.cache\huggingface
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      # Or use Docker volume (slower first run)
      # - model-cache:/root/.cache/huggingface
    runtime: nvidia
    restart: unless-stopped
    profiles: [gpu]

volumes:
  model-cache:
